{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vladimirkadnikov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# joblib.load('NewsBuddy/models/classifier.pkl')\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=russian_stop_words)\n",
    "classifier = joblib.load('/Users/vladimirkadnikov/elbrus/NewsBuddy/models/classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('/Users/vladimirkadnikov/elbrus/NewsBuddy/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(user_id):\n",
    "    predicted_channel_category = []\n",
    "    user_id_to_select = user_id\n",
    "    user_data = news[news['user_id'] == user_id_to_select]  # news заменяем на наш файл с новостями\n",
    "    user_data_grouped = user_data.groupby('channel_name')\n",
    "    for channel_name, channel_data in user_data_grouped:\n",
    "        channel_news_texts = channel_data['publication_text'].tolist()\n",
    "        channel_news_tfidf = tfidf_vectorizer.transform(channel_news_texts)\n",
    "\n",
    "        channel_news_predictions = classifier.predict(channel_news_tfidf)\n",
    "\n",
    "        predicted_channel_category.append(\n",
    "            max(set(channel_news_predictions), key=channel_news_predictions.tolist().count))\n",
    "    return predicted_channel_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggestions(predicted_channel_category, category_to_channels):\n",
    "\n",
    "    unique_categories = set(predicted_channel_category)\n",
    "\n",
    "    recommended_channels = []\n",
    "\n",
    "    if len(unique_categories) == 1:\n",
    "        category = unique_categories.pop()\n",
    "        channels_in_category = category_to_channels.get(category, [])\n",
    "        random.shuffle(channels_in_category)  \n",
    "        recommended_channels = channels_in_category[:4]\n",
    "    elif len(unique_categories) == 2:\n",
    "        for category in unique_categories:\n",
    "            channels_in_category = category_to_channels.get(category, [])\n",
    "            random.shuffle(channels_in_category)\n",
    "            recommended_channels.extend(channels_in_category[:2])\n",
    "    elif len(unique_categories) == 3:\n",
    "        for category in unique_categories:\n",
    "            channels_in_category = category_to_channels.get(category, [])\n",
    "            random.shuffle(channels_in_category)\n",
    "            recommended_channels.extend(channels_in_category[:2])\n",
    "\n",
    "    return recommended_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_channels = {\n",
    "    'новости': [\n",
    "        'https://t.me/breakingmash',\n",
    "        'https://t.me/lentachold',\n",
    "        'https://t.me/astrapress',\n",
    "        'https://t.me/lentachold',\n",
    "        'https://t.me/theinsider',\n",
    "        'https://t.me/otsuka_bld',\n",
    "        'https://t.me/meduzalive',\n",
    "        'https://t.me/guardian',\n",
    "    ],\n",
    "    'юмор': [\n",
    "        'https://t.me/ia_panorama',\n",
    "        'https://t.me/dvachannel',\n",
    "        'https://t.me/pezduzalive',\n",
    "        'https://t.me/mudak',\n",
    "        'https://t.me/cats_cats',\n",
    "        'https://t.me/Reddit',\n",
    "        'https://t.me/paperpublic',\n",
    "        'https://t.me/thedankestmemes',\n",
    "        'https://t.me/memes',\n",
    "        'https://t.me/community_memy',\n",
    "    ],\n",
    "    'технологии': [\n",
    "        'https://t.me/bugnotfeature',\n",
    "        'https://t.me/prostinas',\n",
    "        'https://t.me/it_teech',\n",
    "        'https://t.me/rozetked',\n",
    "        'https://t.me/yandex',\n",
    "        'https://t.me/junior_developer_ua',\n",
    "        'https://t.me/htech_plus',\n",
    "        'https://t.me/cyberfreek',\n",
    "        'https://t.me/python2day',\n",
    "        'https://t.me/addmeto',\n",
    "    ],\n",
    "    'экономика': [\n",
    "        'https://t.me/zeroton',\n",
    "        'https://t.me/guriev_sm',\n",
    "        'https://t.me/financelist',\n",
    "        'https://t.me/proeconomics',\n",
    "        'https://t.me/hoolinomics',\n",
    "        'https://t.me/dohod',\n",
    "        'https://t.me/prime1',\n",
    "        'https://t.me/AK47pfl',\n",
    "        'https://t.me/forbesrussia',\n",
    "        'https://t.me/selfinvestor',\n",
    "    ],\n",
    "    'игры': [\n",
    "        'https://t.me/+VIuvvPWhb-mR4BRq',\n",
    "        'https://t.me/Dota2',\n",
    "        'https://t.me/egs_tg',\n",
    "        'https://t.me/vgtimes',\n",
    "        'https://t.me/stopgamenews',\n",
    "        'https://t.me/PROgame_news',\n",
    "        'https://t.me/GamezTop7',\n",
    "        'https://t.me/gamerbay',\n",
    "        'https://t.me/combobreaker',\n",
    "        'https://t.me/progamedev',\n",
    "    ],\n",
    "    'спорт': [\n",
    "        'https://t.me/championat',\n",
    "        'https://t.me/Match_TV',\n",
    "        'https://t.me/myachPRO',\n",
    "        'https://t.me/sportsru',\n",
    "        'https://t.me/QryaProDucktion',\n",
    "        'https://t.me/sjbodyfit',\n",
    "        'https://t.me/fiztransform',\n",
    "        'https://t.me/sportsmens1',\n",
    "        'https://t.me/runforhealth',\n",
    "        'https://t.me/sportazarto',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 6555020781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_categories(user_id)\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mget_categories\u001b[0;34m(user_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m channel_name, channel_data \u001b[39min\u001b[39;00m user_data_grouped:\n\u001b[1;32m      7\u001b[0m     channel_news_texts \u001b[39m=\u001b[39m channel_data[\u001b[39m'\u001b[39m\u001b[39mpublication_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m----> 8\u001b[0m     channel_news_tfidf \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39;49mtransform(channel_news_texts)\n\u001b[1;32m     10\u001b[0m     channel_news_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(channel_news_tfidf)\n\u001b[1;32m     12\u001b[0m     predicted_channel_category\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     13\u001b[0m         \u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m(channel_news_predictions), key\u001b[39m=\u001b[39mchannel_news_predictions\u001b[39m.\u001b[39mtolist()\u001b[39m.\u001b[39mcount))\n",
      "File \u001b[0;32m~/elbrus/NewsBuddy/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2161\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, raw_documents):\n\u001b[1;32m   2146\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \n\u001b[1;32m   2148\u001b[0m \u001b[39m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[39m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2161\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m, msg\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2163\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtransform(raw_documents)\n\u001b[1;32m   2164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/elbrus/NewsBuddy/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "get_categories(user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
